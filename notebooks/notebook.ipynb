{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25.12.2019\n",
    "### Data Processing\n",
    "*Ro-ee Tal*\n",
    "\n",
    "- Created a custom data transformer to transform Shapefile annotations of dataset into COCO format for compatible use with Detectron2.\n",
    "\n",
    "- There is misalignement in the dataset images and corresponding annotations. For now the conversion from GIS latitude and longitude coordinates to pixel coordinates has been hardcoded, but should be dealt with to correct alignment.\n",
    "\n",
    "- Segment classification has been preserved, but can be modified to restrict segmentation to a single class (i.e. tree) initially as apposed to tree types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02.01.2020\n",
    "### Data Processing\n",
    "*Ro-ee Tal*\n",
    "\n",
    "- The misalignment has now been corrected for, by dynamically using GIS libraries.\n",
    "- This custom data loader also supports multiple datasets (all within a main dataset directory), but for single orthographic images.\n",
    "- These single annotated orthographic images still need to be cut up into fixed sized samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to visualize COCO transformed dataset\n",
    "dataset_path = \"/Users/Ro/Google Drive/UBC_EngCapstone/sample_data/CPT2a-n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-10-cf7558f487ca>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-cf7558f487ca>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    vis = visualizer.draw_dataset_dict(dataset_dicts[0])\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "import cv2\n",
    "from data_processing.register_shapefile_datatset import shapefile_to_coco_dict\n",
    "from detectron2.data import MetadataCatalog\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    classes, dataset_dicts = shapefile_to_coco_dict(dataset_path)\n",
    "except:\n",
    "    print('file open failed')\n",
    "img = cv2.imread(dataset_path+'/CPT2a-n_ortho-resample.tif')\n",
    "visualizer = Visualizer(img[:, :, :], metadata=MetadataCatalog.get(\"CPT2a-n\"), scale=1)\n",
    "vis = visualizer.draw_dataset_dict(dataset_dicts[0])\n",
    "plt.imshow(vis.get_image()[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03.01.2020\n",
    "### Training\n",
    "*Ro-ee Tal*\n",
    "\n",
    "Run 'dummy' training on some annotated orthos to test the pipline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register all Shapefile datasets into COCO for Detectrons\n",
    "\n",
    "import os\n",
    "import detectron2\n",
    "from data.register_shapefile_datatset import shapefile_to_coco_dict\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "dataset_path = \"/Users/Ro/Google Drive/UBC_EngCapstone/sample_data/\"\n",
    "\n",
    "datasets = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d)) and os.path.isdir(os.path.join(dataset_path, d + '/Segments'))]\n",
    "# for dataset in datasets:\n",
    "dataset = datasets[0]\n",
    "if dataset not in DatasetCatalog.list():\n",
    "    classes, dataset_dicts = shapefile_to_coco_dict(os.path.join(dataset_path, dataset))\n",
    "    DatasetCatalog.register(dataset, lambda : dataset_dicts)\n",
    "    MetadataCatalog.get(dataset).set(thing_classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "cfg = get_cfg()\n",
    "# Device -> CPU for now\n",
    "cfg.MODEL.DEVICE = 'cpu'\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (dataset,)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 1\n",
    "# cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # Let training initialize from model zoo\n",
    "# Downloaded manually due to SSL certification error\n",
    "cfg.MODEL.WEIGHTS = os.path.realpath('model_zoo/model_final_f10217.pkl')\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 10    # 300 iterations seems good enough, but you can certainly train longer\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (tree)\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (dataset, )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "import matplotlib.pyplot as plt\n",
    "from data.register_shapefile_datatset import shapefile_to_coco_dict\n",
    "import cv2\n",
    "\n",
    "dataset_path = \"/Users/Ro/Google Drive/UBC_EngCapstone/sample_data/CPT2a-n\"\n",
    "_, dataset_dicts = shapefile_to_coco_dict(dataset_path)\n",
    "im = cv2.imread(dataset_dicts[0][\"file_name\"])\n",
    "outputs = predictor(im)\n",
    "v = Visualizer(im[:, :, :],\n",
    "               metadata=MetadataCatalog.get(\"CPT2a-n\"), \n",
    "               scale=0.8, \n",
    "               instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n",
    ")\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "plt.imshow(v.get_image()[:, :, ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "## 05.01.20\n",
    "### Data Exploration\n",
    "*Tom Ginsberg*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "The first step in the data investigation porcess was to take a look at the polygons in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import shapefile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "sf = shapefile.Reader(\"../datasets/CPT2a-n/Segments/CPT2a-n_dom-poly\")\n",
    "print(sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Let's see what a random sample of 125 polygons look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "shapes = sf.shapes()\n",
    "num_shapes = len(shapes)\n",
    "fig, axes = plt.subplots(5, 25, figsize=(35,10))\n",
    "for ax in axes.ravel():\n",
    "    idx = np.random.randint(num_shapes)\n",
    "    ax.set_title(f'{idx}')\n",
    "    ax.plot(*np.array((shapes[idx].points)).transpose())\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "It looks like the polygons are heavily pixelated. Smoothing them would result in much less data being required to specify shapes and hence faster training times. We use QGIS weighted area vector simplification with a .5m tolerance. \n",
    "\n",
    "#### Simplifed Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "sf = shapefile.Reader(\"../datasets/CPT2a-n/simple/segs_simple\")\n",
    "shapes = sf.shapes()\n",
    "num_shapes = len(shapes)\n",
    "fig, axes = plt.subplots(5, 25, figsize=(35,10))\n",
    "for ax in axes.ravel():\n",
    "    idx = np.random.randint(num_shapes)\n",
    "    ax.set_title(f'{idx}')\n",
    "    ax.plot(*np.array((shapes[idx].points)).transpose())\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Some polygons appear to have strange artifacts. Some examples are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "idxs = [6845, 1968, 6420, 5926]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "for i, idx in enumerate(idxs):\n",
    "    axes[i%2, (i//2)].plot(*np.array((shapes[idx].points)).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "We try the most naive solution just removing the last few points after the polygon completes itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def fix_polygon_tail(polygon):\n",
    "    # First point in polygon\n",
    "    first = polygon[0]\n",
    "    new_poly = []\n",
    "    for i, p in enumerate(polygon):\n",
    "        # keep every point up to the re-occurrence of the first point \n",
    "        new_poly.append(p)\n",
    "        if i > 0 and p == first:\n",
    "            break\n",
    "    return new_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "View the improved polygons with no artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 25, figsize=(35,10))\n",
    "for ax in axes.ravel():\n",
    "    idx = np.random.randint(num_shapes)\n",
    "    ax.set_title(f'{idx}')\n",
    "    ax.plot(*np.array(fix_polygon_tail(shapes[idx].points)).transpose())\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Revisiting the problemactic case from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "idxs = [6845, 1968, 6420, 5926]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "for i, idx in enumerate(idxs):\n",
    "    axes[i%2, (i//2)].plot(*np.array(fix_polygon_tail(shapes[idx].points)).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "## 11.01.20\n",
    "### Data Prep\n",
    "*Tom Ginsberg*\n",
    "\n",
    "Very large images must tiled into smaller images to perform to the specs of common computer vision models. Tiling the image itself is trivial, however a harder task is to tile the training data segments (i.e polygons delineating tree crowns). \n",
    "\n",
    "The following code shows the prototyping of an implementation to tile polygons over a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "We start by reading a randomly generated dataset of 100 polygons over a \\[100 x 100\\] grid.\n",
    "\n",
    "*Generated with Mathematica using the following* \n",
    "```\n",
    "Table[Translate[RandomPolygon[],RandomReal[99, 2]], 100]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with open('resources/polygons.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "data = eval(data)\n",
    "for polygon in data:\n",
    "    polygon.append(polygon[0])\n",
    "polygons = [np.array(polygon) for polygon in data][2:]\n",
    "\n",
    "for polygon in polygons:\n",
    "    plt.plot(*polygon.transpose());\n",
    "    \n",
    "plt.title('Test Polygon Dataset');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Next we compute bounding boxes of polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def bbox(poly):\n",
    "    x, y = poly.transpose()\n",
    "    return [np.min(x), np.min(y), np.max(x), np.max(y)]\n",
    "\n",
    "bboxs = [bbox(poly) for poly in polygons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Now we specify that we want to tile this dataset into \\[40 x 40] tiles with an overlay of 10 in both dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputExpanded": true,
    "outputHidden": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset dimensions\n",
    "x_dim, y_dim = 100, 100\n",
    "\n",
    "# Grid parameters\n",
    "w, h, w_overlay, h_overlay = 40, 40, 10, 10\n",
    "\n",
    "dx, dy = w-w_overlay, h-h_overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Visualize the grid \n",
    "\n",
    "tile_corners = []\n",
    "for y in range(0, y_dim, dy):\n",
    "    for x in range(0, x_dim, dx):    \n",
    "        x_c = x+w if x+w < x_dim else x_dim \n",
    "        y_c = y+h if y+h < y_dim else y_dim\n",
    "        tile_corners.append([x, y, x_c, y_c])\n",
    "        plt.plot([x,x,x_c,x_c,x],[y,y_c,y_c,y,y])\n",
    "        \n",
    "plt.title('Tiling Grid');\n",
    "\n",
    "num_tiles = ceil(x_dim /dx) * ceil(y_dim / dy)\n",
    "print(f'{num_tiles} tiles in grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# a few helper functions\n",
    "\n",
    "def get_bbox_corners(bbox):\n",
    "    xmi, ymi, xma, yma = bbox\n",
    "    return np.array([[xmi,ymi],[xmi,yma],[xma,yma],[xma,ymi],[xmi,ymi]])\n",
    "\n",
    "def if_non_zero(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def box_in_box(contained, container):\n",
    "    \"\"\"\n",
    "    :param container: Rectangle [x min, y min, x_max, y_max]\n",
    "    :param contained: Rectangle [x min, y min, x max, y max]\n",
    "    :return: T/F Is contained in container?\n",
    "    \"\"\"\n",
    "    if min(container[:2]) < 0:\n",
    "        return False\n",
    "    if contained[0] >= container[0] and contained[1] >= container[1] and contained[2] <= container[2] \\\n",
    "            and contained[3] <= container[3]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Select a random polygon and visulaize which grid cells in belongs to out of it's neighboring four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2)\n",
    "poly_num = np.random.randint(0, len(polygons))\n",
    "print(f'Polygon Number: {poly_num}')\n",
    "x_min, y_min, x_max, y_max = bboxs[poly_num]\n",
    "x_pos, y_pos = x_min // dx, y_min // dy\n",
    "for x_shift in [0, dx]:\n",
    "    for y_shift in [0, dy]:\n",
    "        x, y = dx * x_pos - x_shift, dy * y_pos - y_shift\n",
    "        x_c = x+w if x+w < x_dim else x_dim \n",
    "        y_c = y+h if y+h < y_dim else y_dim \n",
    "        \n",
    "        axes[if_non_zero(y_shift),if_non_zero(x_shift-dx)].plot([x,x,x_c,x_c,x],[y,y_c,y_c,y,y])\n",
    "        axes[if_non_zero(y_shift),if_non_zero(x_shift-dx)].plot(*polygons[poly_num].transpose())\n",
    "        \n",
    "        color = 'g' if box_in_box(bboxs[poly_num], [x, y, x_c, y_c]) else 'r'\n",
    "        axes[if_non_zero(y_shift),if_non_zero(x_shift-dx)].plot(*get_bbox_corners(bboxs[poly_num]).transpose(), c=color);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Iterate over all the polygons and assign them to the bins that correspond the the grid tiles that they can be found in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "tiles = [[] for _ in range(num_tiles)]\n",
    "x_tiles = ceil(x_dim / dx)\n",
    "for poly, bbox in zip(polygons, bboxs):\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    x_pos, y_pos = x_min // dx, y_min // dy\n",
    "    for x_shift in [0, dx]:\n",
    "        for y_shift in [0, dy]:\n",
    "            x, y = dx * x_pos - x_shift, dy * y_pos - y_shift\n",
    "            x_c = x+w if x+w < x_dim else x_dim \n",
    "            y_c = y+h if y+h < y_dim else y_dim\n",
    "            \n",
    "            x_pos_c, y_pos_c = \\\n",
    "            x_pos - if_non_zero(x_shift), y_pos - if_non_zero(y_shift)\n",
    "            \n",
    "            if box_in_box(bbox, [x, y, x_c, y_c]):\n",
    "                tiles[int((x_tiles) * y_pos_c + x_pos_c)].append(poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "Visualize the results of the tiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize=(30,10))\n",
    "fig.subplots_adjust(hspace=.35, wspace=.1)\n",
    "x , y = 0, 0\n",
    "for tile_num, (tile, bound, ax) in enumerate(zip(tiles, tile_corners, axes.ravel())):\n",
    "    ax.set_aspect(1)\n",
    "    ax.plot(*get_bbox_corners(bound).transpose())\n",
    "    ax.set_title(f'Tile {tile_num}')\n",
    "    \n",
    "    for poly in tile:\n",
    "        ax.plot(*poly.transpose());\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
